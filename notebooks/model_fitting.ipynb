{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting rank-frequency distribution of Hashtags\n",
    "\n",
    "In this notebook we describe Zipf's law by the 8 generalized models in section III of the paper.\n",
    "\n",
    "We fit the models using our original Zenodo data set and the cumulative data sets in the \"Data\" folder (obtain these by running the aggregate_data notebook). We write out our fitted results such as parameter estimates and log likelihoods to the \"output\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, codecs\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import gzip\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join(os.pardir,'src')))\n",
    "\n",
    "from modules_distributor import fit\n",
    "from general import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_out = '../output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See paper for exact details regarding the models\n",
    "models = ['simple', 'double_powerlaw', 'lognormal', 'naranan', 'expcutoff', 'weibull', 'shifted', 'double_2gammas']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small dataset \n",
    "(1st day, only Hashtags with more than 100 appearances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../Data/\"\n",
    "fout_name = \"combined_data_firstDay.csv\" # name of the output file that contains combined data for 392 days\n",
    "\n",
    "f = open(path + fout_name, \"r\")\n",
    "contents = f.read()\n",
    "f.close()  \n",
    "\n",
    "res_list_temp = contents.split(\"\\n\")\n",
    "\n",
    "all_counts = []\n",
    "\n",
    "for i in range(len(res_list_temp)-1): # last element is empty, we cannot split it by \",\"\n",
    "    split_result = res_list_temp[i].split(\",\")\n",
    "    all_counts.append(int(split_result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the different models to the small dataset, output is:\n",
    "\n",
    "([Model Parameters], -log(Likelihood), number of success of fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple\n",
      "([1.1533865383467519], 8.846853420042262, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ega/git-repos/HashtagsOnTwitter/src/modules_fitting_2exp.py:90: RuntimeWarning: overflow encountered in double_scalars\n",
      "  C = zeta_minmax(gamma1,kmin,km+1) + km**(gamma2-gamma1)*zeta_minmax(gamma2,km+1,kmax)\n",
      "/home/ega/git-repos/HashtagsOnTwitter/src/modules_fitting_2exp.py:90: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  C = zeta_minmax(gamma1,kmin,km+1) + km**(gamma2-gamma1)*zeta_minmax(gamma2,km+1,kmax)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "double_powerlaw\n",
      "([75.76997432617122, 13265.578857606195], 8.280642728782166, 10)\n",
      "lognormal\n",
      "([5.948848643283158, 2.401872522607375], 8.249867348529552, 10)\n",
      "naranan\n",
      "([1.2519828071518573, 5.907264755700497], 8.558211699831606, 10)\n",
      "expcutoff\n",
      "([0.6872449468460942, 0.00016133186370129277], 8.156616332700626, 10)\n",
      "weibull\n",
      "([-0.30815327144784604, 3.795951196760681], 9.465460836853136, 10)\n",
      "shifted\n",
      "([1.6078258074448946, 180.78218712386004], 8.34313356097208, 10)\n",
      "double_2gammas\n",
      "([0.8080437065556514, 24.093825188343484, 12430.672293067993], 8.145755505287767, 10)\n"
     ]
    }
   ],
   "source": [
    "nrep = 10 # number of repetitions\n",
    "\n",
    "for model in models:\n",
    "    res = fit(model = model, counts = all_counts, nrep = nrep)\n",
    "    print(model + \"\\n\" + str(res))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined data for all 392 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../Data/\"\n",
    "fout_name = \"combined_data.csv.gz\" # name of the output file that contains combined data for 392 days\n",
    "\n",
    "f = open(path + fout_name, \"r\")\n",
    "contents = f.read()\n",
    "f.close()  \n",
    "\n",
    "res_list_temp = contents.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_counts = []\n",
    "\n",
    "for i in range(len(res_list_temp)-1): # last element is empty, we cannot split it by \",\"\n",
    "    split_result = res_list_temp[i].split(\",\")\n",
    "    all_counts.append(int(split_result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output here only shows the fitted results after 1 repetition. i.e: nrep = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrep = 10 # number of repetitions\n",
    "\n",
    "# Writing results to a file\n",
    "fout = open(path_out + \"combined_data\" + \"-nrep\" + str(nrep) + \".txt\", \"w\")\n",
    "\n",
    "for model in models:\n",
    "    res = fit(model = model, counts = all_counts, nrep = nrep)\n",
    "    print(model + \"\\n\" + str(res))\n",
    "    fout.write(str(res) + \"\\n\")\n",
    "    \n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulative data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consecutive days in temporal order. Eg: 2 days in a row, 7 days in a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in = '../Data/Cumulative/day/'\n",
    "folder_out = 'Cumulative/Zipf_fitted/'\n",
    "\n",
    "# IMPORTANT: These need to match the given_intervals in aggregate_data.ipynb\n",
    "given_intervals_temp = np.power(2, [0, 1, 2, 3, 4, 5, 6, 7 , 8])\n",
    "given_intervals = np.concatenate(([0], given_intervals_temp, [num_of_files]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(path_out + folder_out): \n",
    "        os.makedirs(path_out + folder_out) #if the path doesn't exist, we create it. Otherwise, we carry on.\n",
    "\n",
    "\n",
    "for i in range(len(given_intervals)-1):\n",
    "    f_in_name = 'data_' + str(i) +  '_cum' # name of the input file (contains the data)\n",
    "\n",
    "    f = open(path_in + f_in_name + \".txt\", \"r\")\n",
    "    contents = f.read()\n",
    "    f.close()\n",
    "\n",
    "    res_list_temp = contents.split(\"\\n\")\n",
    "    \n",
    "    hashtags_temp = []\n",
    "    counts_temp = []\n",
    "\n",
    "    for j in range(len(res_list_temp)-1):\n",
    "        split_result = res_list_temp[j].split(\",\")\n",
    "        hashtags_temp.append(split_result[0])\n",
    "        counts_temp.append(int(split_result[1]))\n",
    "        \n",
    "    # Model fitting and writing results to a file\n",
    "    nrep = 10\n",
    "\n",
    "    fout = open(path_out + folder_out + \"fit\" + str(i) + \"_cum\" +  \"-nrep\" + str(nrep) + \".txt\", \"w\")\n",
    "\n",
    "    for model in models:\n",
    "        res = fit(model = model, counts = counts_temp, nrep = nrep)\n",
    "        fout.write(str(res) + \"\\n\")\n",
    "    \n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are easily extended to fitting models for every day, minute or hour. We can use data from the original Zenodo data base and from the \"Data\" folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
