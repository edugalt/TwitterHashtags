{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate usage of Hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our Zenodo data set, we combine files to accummulate data over all 392 days and over smaller subintervals. We combine over smaller intervals in the temporal order of days and randomly. The shuffled cumulative data is used in the Heaps and Taylor's law analysis (Section IV) of paper. Accumlated data and randomised/shuffled data will be stored in a new folder called \"Data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, codecs\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join(os.pardir,'src')))\n",
    "\n",
    "from modules_distributor import fit\n",
    "from general import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_path = \"../Data/hashtags_frequency_day/\"\n",
    "hour_path = \"../Data/hashtags_frequency_hour/\"\n",
    "min_path = \"../Data/hashtags_frequency_minutes/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all the zipped day files\n",
    "list_of_files = []\n",
    "\n",
    "list_of_files = get_zipped_files(day_path)\n",
    "num_of_files = len(list_of_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining the hashtag data from all 392 days in our data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "\n",
    "for i in range(num_of_files):\n",
    "    hashtag_dict = get_data(day_path, list_of_files[i], \"day\")\n",
    "\n",
    "    hashtags = list(hashtag_dict.keys())\n",
    "    counts = list(hashtag_dict.values())\n",
    "\n",
    "    for i in range(len(hashtags)):\n",
    "        if hashtags[i] in data_dict.keys():\n",
    "            data_dict[hashtags[i]] += counts[i] \n",
    "            # if the hashtag occured on a previous dy, we add on the count\n",
    "        else:\n",
    "            data_dict[hashtags[i]] = counts[i] \n",
    "            # if this is the first time the hashtag has occurred, we create a new key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing out results to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_out = '../Data/'\n",
    "fout_name = 'combined_data4' #the name of the output file\n",
    "\n",
    "if not os.path.exists(path_out): # create folder if it doesn't already exist\n",
    "    os.makedirs(path_out)\n",
    "    \n",
    "fout = open(path_out + fout_name + \".csv.gz\", \"w\")\n",
    "\n",
    "for hashtag, count in data_dict.items():\n",
    "    fout.write(str(hashtag) + \",\" + str(count) + \"\\n\")\n",
    "\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining data (cumulative data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we gradually accumulate the data, starting from just the first day, then the first two days, first four days, first eight days. In general, the first 2^n days up until all our entire data set of 392 days/files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The intervals at which we accumulate. We can refine the intervals to make them smaller, \n",
    "# eg: first 1 day -> first 2 days -> first 3 days -> ... first n days\n",
    "# but this will more compuationally and memory storage expensive.\n",
    "given_intervals_temp = np.power(2, [0, 1, 2, 3, 4, 5, 6, 7 , 8])\n",
    "given_intervals = np.concatenate(([0], given_intervals_temp, [num_of_files]))\n",
    "\n",
    "# We export our results into a file, so we don't have to run the double for loop again\n",
    "path_out = '../Data/Cumulative/day/'\n",
    "\n",
    "# If this folder doesn't exist, we create it\n",
    "if not os.path.exists(path_out): \n",
    "    os.makedirs(path_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function which combines the hashtag data from multiple files. We sum up the hashtag type usage\n",
    "# across different files which are found in the path_in file path. The time_format should either\n",
    "# be day, hour or minute. This is because the first column for days is formatted differently to\n",
    "# minutes and hours, so the get_data() function behaves differently as well. The arguments start\n",
    "# and given_intervals is the file/time when we start from and we accumulate until we have reached\n",
    "# the time/number of files given by given_intervals. We write out the output a file path given by\n",
    "# path_out and is in the format of the Zenodo data set. The hashtag followed by a comma and then\n",
    "# the hashtag count.\n",
    "\n",
    "def get_cum_data(path_in, files, time_format, given_intervals, start, path_out):\n",
    "    # Indices of for loops are set such that we update data_dict based on the previous iteration, \n",
    "    # saving compuational time.\n",
    "    data_dict = {}\n",
    "\n",
    "    for i in range(0, len(given_intervals)-1):\n",
    "        indices_i = range(0, given_intervals[i])\n",
    "\n",
    "        # Combining hashtags and counts (cumulatively)\n",
    "        for index in indices_i:\n",
    "            hashtag_dict = get_data(path_in, files[i], time_format)\n",
    "            hashtags = list(hashtag_dict.keys())\n",
    "            counts = list(hashtag_dict.values())\n",
    "\n",
    "            for j in range(len(hashtags)):\n",
    "                if hashtags[j] in data_dict.keys():\n",
    "                    data_dict[hashtags[j]] += counts[j] \n",
    "                    # if the hashtag occurred on a previous day, we add the counts\n",
    "                else:\n",
    "                    data_dict[hashtags[j]] = counts[j] \n",
    "                    # if this is the first time the hashtag occurred, we create a new key\n",
    "\n",
    "        # writing results to desired folder and file\n",
    "        fout_name = 'data_' + str(i) +  '_cum' + \"_start_\" + str(start) # name of output file\n",
    "        fout = open(path_out + fout_name + \".txt\", \"w\")\n",
    "\n",
    "        for hashtag, count in data_dict.items():\n",
    "            fout.write(str(hashtag) + \",\" + str(count) + \"\\n\")\n",
    "        fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start accumulating from different start points\n",
    "start_list = list(range(0, num_of_files - 16, 40))[1:10]\n",
    "\n",
    "for start in start_list:\n",
    "    get_cum_data(day_path, list_of_files, \"day\", given_intervals, start, path_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding type and token count using cumulative data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function counts the number of hashtag types and tokens in a given interval of time and a given\n",
    "# start point. The interval of time can be on the minute, hour or day scale and is determined by \n",
    "# the path_in argument. The path_in argument is the source of the data and the path_out is the \n",
    "# folder where we write out our count. The output is a text file with the first column \n",
    "# corresponding to the type count and the second column is the token count.\n",
    "\n",
    "def count_types_tokens(given_intervals, start, path_in, path_out):\n",
    "    if not os.path.exists(path_out): \n",
    "        os.makedirs(path_out)\n",
    "        \n",
    "    fout = open(path_out + \"types_tokens_count_start_\" + str(start) + \".txt\", \"w\")\n",
    "\n",
    "    for i in range(len(given_intervals)-1):\n",
    "        f_in_name = 'data_' + str(i) +  '_cum' #the name of the input file (contains the data)\n",
    "\n",
    "        f = open(path_in + f_in_name + \".txt\", \"r\")\n",
    "        contents = f.read()\n",
    "        f.close()\n",
    "\n",
    "        res_list_temp = contents.split(\"\\n\")\n",
    "\n",
    "        hashtags_temp = []\n",
    "        counts_temp = []\n",
    "\n",
    "        for j in range(len(res_list_temp)-1): # last element is empty, we cannot split it by \",\"\n",
    "            split_result = res_list_temp[j].split(\",\")\n",
    "            hashtags_temp.append(split_result[0])\n",
    "            counts_temp.append(int(split_result[1]))\n",
    "\n",
    "        fout.write(str(len(hashtags_temp)) + \",\" + str(sum(counts_temp)) + \"\\n\")\n",
    "\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: it would be more efficient to count types and tokens when we are actually accumulating above\n",
    "given_intervals_temp = np.power(2, [0, 1, 2, 3, 4, 5, 6, 7 , 8])\n",
    "given_intervals = np.concatenate(([0], given_intervals_temp, [392]))\n",
    "\n",
    "path_in = '../Data/Cumulative/day/'\n",
    "path_out = '../output/Cumulative/day/'\n",
    "for start in start_list:\n",
    "    count_types_tokens(given_intervals, start, path_in, path_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulative (minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accumulating using our minute data in temporal order. We start from various different initial points and compute the mean and standrad deviation. Will be used for Taylor's and Heaps' Law plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_out = '../Data/Cumulative/min/'\n",
    "\n",
    "if not os.path.exists(path_out): \n",
    "    os.makedirs(path_out)\n",
    "\n",
    "given_intervals = np.concatenate(([0], np.power(2, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])))\n",
    "\n",
    "# Getting all the zipped files\n",
    "list_of_files = get_zipped_files(min_path)\n",
    "num_of_files = len(list_of_files)\n",
    "\n",
    "start_list = list(range(3000, num_of_files, 3000)) #change this to vary the start point\n",
    "\n",
    "for start in start_list:\n",
    "    get_cum_data(min_path, list_of_files, \"min\", given_intervals, start, path_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding type and token count using cumulative data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: it would be more efficient to count types and tokens when we are actually accumulating above\n",
    "path_in = '../Data/Cumulative/min/'\n",
    "path_out = '../output/Cumulative/min/'\n",
    "\n",
    "for start in start_list:\n",
    "    count_types_tokens(given_intervals, start, path_in, path_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulative (hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_out = '../Data/Cumulative/hour/'\n",
    "\n",
    "if not os.path.exists(path_out): \n",
    "    os.makedirs(path_out)\n",
    "\n",
    "given_intervals = np.concatenate(([0], np.power(2, [0, 1, 2, 3, 4, 5, 6, 7])))\n",
    "\n",
    "# Getting all hour files\n",
    "list_of_files = get_zipped_files(hour_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_list = list(range(1000, num_of_files, 1000)) #change this to vary the start point\n",
    "\n",
    "for start in start_list:\n",
    "    get_cum_data(hour_path, list_of_files, \"hour\", given_intervals, start, path_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding type and token count using cumulative data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in = '../Data/Cumulative/hour/'\n",
    "path_out = '../output/Cumulative/hour/'\n",
    "\n",
    "for start in start_list:\n",
    "    count_types_tokens(given_intervals, start, path_in, path_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomised cumulative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accumulating our data based on a randomised order. Used for Taylor's and Heaps' Law plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2020) # for repoducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomised cumulative (minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_out = '../Data/Random_Cumulative/min/'\n",
    "\n",
    "if not os.path.exists(path_out): \n",
    "    os.makedirs(path_out)\n",
    "    \n",
    "given_intervals = np.concatenate(([0], np.power(2, range(0, 12))))\n",
    "\n",
    "list_of_files = get_zipped_files(min_path)\n",
    "num_of_files = len(list_of_files)\n",
    "\n",
    "N = 10 # number of different times we accumulate, used to find mean and SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for start in range(N):\n",
    "    random_order = random.sample(range(num_of_files), given_intervals[-1]) \n",
    "    # generate random integers so that we have enough to accumulate based on the final value of given_intervals.\n",
    "    \n",
    "    get_cum_data(min_path, list_of_files, \"min\", given_intervals, start, path_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding type and token count using cumulative data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in = '../Data/Random_Cumulative/min/'\n",
    "path_out = '../output/Random_Cumulative/min/'\n",
    "\n",
    "if not os.path.exists(path_out): \n",
    "    os.makedirs(path_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for start in range(N):\n",
    "    count_types_tokens(given_intervals, start, path_in, path_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomised cumulative (hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_out = '../Data/Random_Cumulative/hour/'\n",
    "\n",
    "if not os.path.exists(path_out): \n",
    "    os.makedirs(path_out)\n",
    "    \n",
    "given_intervals = np.concatenate(([0], np.power(2, range(0, 11))))\n",
    "\n",
    "list_of_files = get_zipped_files(hour_path)\n",
    "num_of_files = len(list_of_files)\n",
    "\n",
    "N = 10 # number of different times we accumulate, used to find mean and SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for start in range(N):\n",
    "    random_order = random.sample(range(num_of_files), given_intervals[-1]) \n",
    "    # generate random integers\n",
    "    \n",
    "    get_cum_data(hour_path, list_of_files, \"hour\", given_intervals, start, path_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding type and token count using cumulative data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in = '../Data/Random_Cumulative/hour/'\n",
    "path_out = '../output/Random_Cumulative/hour/'\n",
    "\n",
    "if not os.path.exists(path_out): \n",
    "    os.makedirs(path_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for start in range(N):\n",
    "    count_types_tokens(given_intervals, start, path_in, path_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomised cumulative (day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_out = '../Data/Random_Cumulative/day/'\n",
    "\n",
    "if not os.path.exists(path_out): \n",
    "    os.makedirs(path_out)\n",
    "    \n",
    "given_intervals = np.concatenate(([0], np.power(2, range(0, 9))))\n",
    "\n",
    "list_of_files = get_zipped_files(day_path)\n",
    "num_of_files = len(list_of_files)\n",
    "\n",
    "N = 10 #number of times we accumulate in order to find mean/SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for start in range(N):\n",
    "    random_order = random.sample(range(num_of_files), given_intervals[-1]) # random integers\n",
    "    get_cum_data(day_path, list_of_files, \"day\", given_intervals, start, path_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in = '../Data/Random_Cumulative/day/'\n",
    "path_out = '../output/Random_Cumulative/day/'\n",
    "\n",
    "if not os.path.exists(path_out): \n",
    "    os.makedirs(path_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for start in range(N):\n",
    "    count_types_tokens(given_intervals, start, path_in, path_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
